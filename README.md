# ToggleMaster - Fase 03 FIAP

Sistema completo de gerenciamento de feature flags com arquitetura de microsserviÃ§os, infraestrutura como cÃ³digo (Terraform), Kubernetes (EKS), GitOps com ArgoCD e pipelines DevSecOps.

---

## ğŸ“‹ Ãndice

- [Quick Start](#-quick-start-5-minutos)
- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#ï¸-arquitetura)
- [MicrosserviÃ§os](#-microsserviÃ§os)
- [GitOps com ArgoCD](#-gitops-com-argocd)
- [Infraestrutura Terraform](#ï¸-infraestrutura-terraform)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Comandos Ãšteis](#-comandos-Ãºteis)
- [Troubleshooting](#-troubleshooting)

---

## âš¡ Quick Start (5 minutos)

### 1ï¸âƒ£ Deploy da Infraestrutura

```bash
cd terraform
terraform init
terraform plan -out=tfplan
terraform apply tfplan
```

### 2ï¸âƒ£ Build e Push das Imagens

```bash
# Configurar AWS CLI
export AWS_ACCESS_KEY_ID=<your-key>
export AWS_SECRET_ACCESS_KEY=<your-secret>
export AWS_DEFAULT_REGION=us-east-1

# Build e push para ECR
./scripts/build-all-services.sh
```

### 3ï¸âƒ£ Instalar ArgoCD

```bash
# Configurar kubectl
aws eks update-kubeconfig --name togglemaster --region us-east-1

# Instalar ArgoCD
./gitops/argocd/install.sh

# Ou manualmente:
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
```

### 4ï¸âƒ£ Obter Credenciais do ArgoCD

```bash
# Usando script auxiliar
./scripts/gitops-manager.sh credentials

# Ou manualmente
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
kubectl get svc argocd-server -n argocd
```

### 5ï¸âƒ£ Deploy dos ServiÃ§os

```bash
# Deploy todos os serviÃ§os
./scripts/gitops-manager.sh deploy

# Verificar status
./scripts/gitops-manager.sh status
kubectl get pods -n togglemaster
```

** Pronto! Acesse o ArgoCD UI para monitorar seus serviÃ§os.**

---

##  VisÃ£o Geral

ToggleMaster Ã© uma plataforma empresarial de feature flags que permite:
- âœ… Gerenciamento centralizado de feature flags
- âœ… AvaliaÃ§Ã£o de flags em tempo real com cache Redis
- âœ… Targeting avanÃ§ado de usuÃ¡rios e segmentaÃ§Ã£o
- âœ… Analytics e monitoramento em tempo real
- âœ… AutenticaÃ§Ã£o e autorizaÃ§Ã£o JWT
- âœ… Deploy automÃ¡tico com GitOps (ArgoCD)
- âœ… CI/CD completo com GitHub Actions
- âœ… SeguranÃ§a integrada (Trivy, gosec, bandit)

---

##  Arquitetura

### **MicrosserviÃ§os (5 serviÃ§os):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AWS Cloud (EKS)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Auth Service â”‚  â”‚ Flag Service â”‚  â”‚ Eval Service â”‚     â”‚
â”‚  â”‚   (Go)       â”‚  â”‚   (Python)   â”‚  â”‚   (Go)       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                 â”‚                  â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚  PostgreSQL  â”‚  â”‚  Redis Cache  â”‚    â”‚
â”‚  â”‚  (RDS)      â”‚  â”‚   (RDS)      â”‚  â”‚ (ElastiCache) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Targeting Serviceâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤Analytics Service â”‚        â”‚
â”‚  â”‚   (Python)       â”‚  Queue  â”‚    (Python)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                             â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  PostgreSQL   â”‚            â”‚    DynamoDB     â”‚         â”‚
â”‚  â”‚    (RDS)      â”‚            â”‚   (NoSQL)       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚           Amazon SQS (Message Queue)         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Ingress (Load Balancer)       â”‚
         â”‚  analytics-service.togglemaster.io â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Stack TecnolÃ³gico:**

- **Container Orchestration:** Kubernetes (Amazon EKS)
- **Infrastructure as Code:** Terraform
- **Container Registry:** Amazon ECR
- **Databases:** 
  - PostgreSQL (Amazon RDS) para Auth, Flag, Targeting
  - Redis (Amazon ElastiCache) para cache
  - DynamoDB para analytics
- **Message Queue:** Amazon SQS
- **CI/CD:** GitHub Actions
- **GitOps:** ArgoCD
- **Security:** Trivy, Bandit, gosec, golangci-lint

---

##  MicrosserviÃ§os

### **1. Auth Service** (Go)
- **Porta:** 8001
- **FunÃ§Ã£o:** AutenticaÃ§Ã£o e geraÃ§Ã£o de tokens JWT
- **Database:** PostgreSQL (RDS)
- **Endpoints:**
  - `POST /auth/login` - Login de usuÃ¡rios
  - `POST /auth/register` - Registro de novos usuÃ¡rios
  - `GET /auth/validate` - ValidaÃ§Ã£o de token

### **2. Flag Service** (Python)
- **Porta:** 8002
- **FunÃ§Ã£o:** CRUD de feature flags
- **Database:** PostgreSQL (RDS)
- **Endpoints:**
  - `GET /flags` - Listar flags
  - `POST /flags` - Criar flag
  - `PUT /flags/:id` - Atualizar flag
  - `DELETE /flags/:id` - Deletar flag

### **3. Evaluation Service** (Go)
- **Porta:** 8004
- **FunÃ§Ã£o:** AvaliaÃ§Ã£o de flags para usuÃ¡rios
- **Cache:** Redis (ElastiCache)
- **Queue:** Amazon SQS (eventos de avaliaÃ§Ã£o)
- **Endpoints:**
  - `POST /evaluate` - Avaliar flag para usuÃ¡rio
  - `GET /evaluate/bulk` - AvaliaÃ§Ã£o em lote

### **4. Targeting Service** (Python)
- **Porta:** 8003
- **FunÃ§Ã£o:** Regras de targeting de usuÃ¡rios
- **Database:** PostgreSQL (RDS)
- **Endpoints:**
  - `GET /targeting/rules` - Listar regras
  - `POST /targeting/rules` - Criar regra
  - `POST /targeting/match` - Verificar match de usuÃ¡rio

### **5. Analytics Service** (Python)
- **Porta:** 8005
- **FunÃ§Ã£o:** Coleta e anÃ¡lise de eventos
- **Database:** DynamoDB
- **Queue Consumer:** Amazon SQS
- **Endpoints:**
  - `GET /analytics/stats` - EstatÃ­sticas de uso
  - `GET /analytics/events` - Eventos registrados
  - `POST /analytics/query` - Query customizada

---

##  Infraestrutura

### **Recursos AWS Provisionados:**

#### **Compute:**
- âœ… Amazon EKS Cluster (Kubernetes 1.29)
- âœ… Node Group: 2x t3.medium (min: 1, max: 4)

#### **Databases:**
- âœ… 3x PostgreSQL RDS instances (db.t3.medium)
  - auth-service DB
  - flag-service DB
  - targeting-service DB
- âœ… Redis ElastiCache (cache.t3.micro)
- âœ… DynamoDB table (ToggleMasterAnalytics)

#### **Container Registry:**
- âœ… 5x ECR repositories (um por serviÃ§o)

#### **Networking:**
- âœ… VPC customizada (10.0.0.0/16)
- âœ… 2x Public Subnets
- âœ… 2x Private Subnets
- âœ… NAT Gateway
- âœ… Internet Gateway
- âœ… Security Groups

#### **Message Queue:**
- âœ… Amazon SQS Queue

### **Terraform Modules:**

```
terraform/
â”œâ”€â”€ main.tf           # Main configuration
â”œâ”€â”€ providers.tf      # AWS provider
â”œâ”€â”€ networking.tf     # VPC, subnets, NAT
â”œâ”€â”€ eks.tf           # EKS cluster
â”œâ”€â”€ database.tf      # RDS instances
â”œâ”€â”€ nosql.tf         # DynamoDB
â”œâ”€â”€ messaging.tf     # SQS
â”œâ”€â”€ registry.tf      # ECR repositories
â”œâ”€â”€ variables.tf     # Input variables
â””â”€â”€ outputs.tf       # Output values
```

---

##  CI/CD & DevSecOps

### **Pipeline Stages:**

Cada microsserviÃ§o possui um pipeline completo com 4 jobs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Build & Test â”‚
â”‚  - Compile      â”‚
â”‚  - Unit Tests   â”‚
â”‚  - Docker Build â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Linter       â”‚
â”‚  - flake8/Go    â”‚
â”‚  - Code Quality â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Security     â”‚
â”‚  - SAST         â”‚
â”‚  - SCA          â”‚
â”‚  âš ï¸ Block CRIT  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Docker Push  â”‚
â”‚  - Build Image  â”‚
â”‚  - Scan Image   â”‚
â”‚  - Push to ECR  â”‚
â”‚  - Update GitOpsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Security Layers:**

1. **Linting:** flake8 (Python), golangci-lint (Go)
2. **SAST:** bandit (Python), gosec (Go)
3. **SCA:** Trivy filesystem scan
4. **Container Security:** Trivy image scan
5. **Blocking:** Pipeline fails on CRITICAL vulnerabilities

### **Workflows:**

- [analytics-service.yml](.github/workflows/analytics-service.yml)
- [auth-service.yml](.github/workflows/auth-service.yml)
- [evaluation-service.yml](.github/workflows/evaluation-service.yml)
- [flag-service.yml](.github/workflows/flag-service.yml)
- [targeting-service.yml](.github/workflows/targeting-service.yml)

---

##  GitOps com ArgoCD

### **Arquitetura GitOps:**

```
Developer â†’ GitHub â†’ CI Pipeline â†’ ECR
                â†“
          Update GitOps
                â†“
            ArgoCD â† monitors Git
                â†“
          Deploy to EKS
                â†“
     5 Microservices Running
```

### **Estrutura GitOps:**

```
gitops/
â”œâ”€â”€ apps/                      # ArgoCD Applications
â”‚   â”œâ”€â”€ analytics-service.yaml
â”‚   â”œâ”€â”€ auth-service.yaml
â”‚   â”œâ”€â”€ evaluation-service.yaml
â”‚   â”œâ”€â”€ flag-service.yaml
â”‚   â””â”€â”€ targeting-service.yaml
â”‚
â”œâ”€â”€ manifests/                 # Kubernetes manifests (Single Source of Truth)
â”‚   â”œâ”€â”€ namespace/
â”‚   â”œâ”€â”€ ingress/
â”‚   â”œâ”€â”€ analytics-service/
â”‚   â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”‚   â”œâ”€â”€ secret.yaml
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ auth-service/
â”‚   â”œâ”€â”€ evaluation-service/
â”‚   â”œâ”€â”€ flag-service/
â”‚   â””â”€â”€ targeting-service/
â”‚
â””â”€â”€ argocd/
    â”œâ”€â”€ install.sh         # InstalaÃ§Ã£o automatizada
    â””â”€â”€ README.md
```

### **Fluxo de Deploy AutomÃ¡tico:**

1. **Code Change**: Developer faz push de cÃ³digo para `main`
2. **CI Pipeline**: GitHub Actions executa build, test, security scans
3. **Image Build**: Docker image criada e enviada para ECR com tag SHA
4. **GitOps Update**: Workflow atualiza tag da imagem em `gitops/manifests/`
5. **ArgoCD Sync**: ArgoCD detecta mudanÃ§a e sincroniza automaticamente
6. **Kubernetes Deploy**: ArgoCD aplica manifestos no cluster EKS
7. **Verification**: Health checks validam deploy bem-sucedido

### **InstalaÃ§Ã£o do ArgoCD:**

```bash
# Via script (recomendado)
./gitops/argocd/install.sh

# Ou manualmente
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Expor via LoadBalancer
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# Obter senha
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# Obter URL
kubectl get svc argocd-server -n argocd
```

### **Configurando ArgoCD Applications:**

Quando fizer push do repositÃ³rio para GitHub, atualize as ArgoCD Applications:

```yaml
# gitops/apps/auth-service.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: auth-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/<seu-usuario>/<seu-repo>.git
    targetRevision: main
    path: gitops/manifests/auth-service
  destination:
    server: https://kubernetes.default.svc
    namespace: togglemaster
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

Aplicar:
```bash
kubectl apply -f gitops/apps/
```

---

##  CI/CD Pipeline

### **Pipeline Stages:**

Cada microsserviÃ§o possui um pipeline completo:

```yaml
jobs:
  build_test:    # Build, compile, test
  lint:          # Code quality (flake8/golangci-lint)
  security:      # Security scans (gosec/bandit + Trivy)
  push_ecr:      # Build image â†’ Push to ECR
  update_gitops: # Update image tag in GitOps â†’ Triggers ArgoCD
```

### **Exemplo de Commit AutomÃ¡tico:**

ApÃ³s push de cÃ³digo, o pipeline:
1. Faz build da imagem com tag SHA (ex: `abc1234`)
2. Push para ECR
3. Atualiza automaticamente `gitops/manifests/auth-service/deployment.yaml`:
   ```yaml
   image: 913430344673.dkr.ecr.us-east-1.amazonaws.com/togglemaster/auth-service:abc1234
   ```
4. Faz commit: ` Update auth-service image to abc1234`
5. ArgoCD detecta e faz deploy automÃ¡tico

### **Security Layers:**

1. **Linting:** flake8 (Python), golangci-lint (Go)
2. **SAST:** bandit (Python), gosec (Go)
3. **SCA:** Trivy filesystem scan
4. **Container Security:** Trivy image scan
5. **Blocking:** Pipeline fails on CRITICAL vulnerabilities

---

##  Comandos Ãšteis

### **Script GitOps Manager:**

```bash
# Configurar kubeconfig
./scripts/gitops-manager.sh configure

# Ver credenciais do ArgoCD
./scripts/gitops-manager.sh credentials

# Ver status do cluster e serviÃ§os
./scripts/gitops-manager.sh status

# Deploy todos os serviÃ§os
./scripts/gitops-manager.sh deploy

# Ver logs de um serviÃ§o
./scripts/gitops-manager.sh logs auth-service

# Reiniciar um serviÃ§o
./scripts/gitops-manager.sh restart auth-service

# Recriar ECR secret
./scripts/gitops-manager.sh ecr-secret

# Port-forward ArgoCD para localhost
./scripts/gitops-manager.sh port-forward
```

### **Kubectl Direto:**

```bash
# Ver pods
kubectl get pods -n togglemaster

# Ver todos os recursos
kubectl get all -n togglemaster

# Ver logs
kubectl logs -n togglemaster -l app=auth-service --tail=50

# Logs em tempo real
kubectl logs -f -n togglemaster deployment/auth-service

# Descrever pod
kubectl describe pod <pod-name> -n togglemaster

# Executar comando em pod
kubectl exec -it <pod-name> -n togglemaster -- sh

# Port-forward para um serviÃ§o
kubectl port-forward -n togglemaster svc/auth-service 8001:8001

# Ver eventos
kubectl get events -n togglemaster --sort-by='.lastTimestamp'

# Ver configuraÃ§Ãµes
kubectl get configmap -n togglemaster
kubectl get secret -n togglemaster

# Escalar deployment
kubectl scale deployment/auth-service --replicas=3 -n togglemaster

# Ver HPA
kubectl get hpa -n togglemaster
```

### **Build e Deploy:**

```bash
# Build todas as imagens e push para ECR
./scripts/build-all-services.sh

# Build um serviÃ§o especÃ­fico
cd Kubernetes/auth-service/auth-service
docker build -t auth-service:latest .

# Login no ECR
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin \
  913430344673.dkr.ecr.us-east-1.amazonaws.com

# Tag e push
docker tag auth-service:latest \
  913430344673.dkr.ecr.us-east-1.amazonaws.com/togglemaster/auth-service:latest
docker push 913430344673.dkr.ecr.us-east-1.amazonaws.com/togglemaster/auth-service:latest
```

### **Terraform:**

```bash
# Ver outputs
cd terraform
terraform output

# Ver outputs em JSON
terraform output -json

# Ver estado
terraform state list

# Refresh state
terraform refresh

# Plan com target especÃ­fico
terraform plan -target=aws_eks_cluster.main

# Apply com auto-approve
terraform apply -auto-approve
```

### **Monitoramento:**

```bash
# Ver recursos consumidos
kubectl top nodes
kubectl top pods -n togglemaster

# MÃ©tricas de um pod especÃ­fico
kubectl describe pod <pod-name> -n togglemaster | grep -A 5 "Limits\|Requests"

# Ver status de health
kubectl get pods -n togglemaster -o wide

# Restart deployment (recreate pods)
kubectl rollout restart deployment/auth-service -n togglemaster

# Ver histÃ³rico de rollouts
kubectl rollout history deployment/auth-service -n togglemaster

# Rollback
kubectl rollout undo deployment/auth-service -n togglemaster
```

---

## ğŸ” Troubleshooting

### **Pods em CrashLoopBackOff:**

**Causa Comum**: Problemas de conectividade com RDS/Redis/SQS

```bash
# Ver logs do pod
kubectl logs -n togglemaster <pod-name>

# Verificar eventos
kubectl describe pod <pod-name> -n togglemaster

# Exemplo de erro: "no pg_hba.conf entry"
# SoluÃ§Ã£o: Ajustar security group do RDS para permitir trÃ¡fego do EKS
```

**SoluÃ§Ã£o para RDS:**
1. Obter security group dos nodes do EKS:
   ```bash
   aws eks describe-cluster --name togglemaster --query "cluster.resourcesVpcConfig.clusterSecurityGroupId"
   ```
2. Adicionar ingress rule no security group do RDS permitindo trÃ¡fego da porta 5432 do security group do EKS

### **ImagePullBackOff:**

**Causa**: Problema ao puxar imagem do ECR

```bash
# Verificar secret
kubectl get secret ecr-secret -n togglemaster -o yaml

# Recriar secret
kubectl delete secret ecr-secret -n togglemaster
kubectl create secret docker-registry ecr-secret \
  --docker-server=913430344673.dkr.ecr.us-east-1.amazonaws.com \
  --docker-username=AWS \
  --docker-password=$(aws ecr get-login-password --region us-east-1) \
  --namespace=togglemaster

# Ou usar o script
./scripts/gitops-manager.sh ecr-secret
```

### **ServiÃ§o nÃ£o responde:**

```bash
# Verificar se o pod estÃ¡ rodando
kubectl get pods -n togglemaster -l app=<service-name>

# Verificar service
kubectl get svc -n togglemaster <service-name>

# Testar conectividade interna
kubectl run -it --rm debug --image=busybox --restart=Never -n togglemaster -- sh
wget -O- http://auth-service:8001/health

# Reiniciar deployment
./scripts/gitops-manager.sh restart <service-name>
```

### **ArgoCD nÃ£o sincroniza:**

```bash
# Verificar status da aplicaÃ§Ã£o
kubectl get applications -n argocd

# Ver detalhes
kubectl describe application auth-service -n argocd

# ForÃ§ar sincronizaÃ§Ã£o (se usando ArgoCD CLI)
argocd app sync auth-service

# Ou via UI: Clicar em "Sync" â†’ "Synchronize"
```

### **Erro de permissÃ£o AWS:**

```bash
# Verificar credenciais AWS
aws sts get-caller-identity

# Verificar se tem acesso ao ECR
aws ecr describe-repositories --region us-east-1

# Verificar se tem acesso ao EKS
aws eks describe-cluster --name togglemaster --region us-east-1
```

### **Pod fica em Pending:**

```bash
# Ver por que estÃ¡ pending
kubectl describe pod <pod-name> -n togglemaster

# Causas comuns:
# - Recursos insuficientes: Aumentar nodes ou reduzir requests
# - ImagePullBackOff: Ver seÃ§Ã£o acima
# - PVC nÃ£o bound: Verificar PersistentVolumeClaims
```

### **Logs Ãºteis para debug:**

```bash
# Logs do kubelet (nos workers)
kubectl logs -n kube-system -l component=kubelet

# Logs do scheduler
kubectl logs -n kube-system -l component=kube-scheduler

# Logs do controller manager
kubectl logs -n kube-system -l component=kube-controller-manager

# Eventos do cluster
kubectl get events --all-namespaces --sort-by='.lastTimestamp'
```
### **Destroy Infrastructure:**

```bash
cd terraform

# Destroy all AWS resources
terraform destroy

# Or destroy specific resources
terraform destroy -target=aws_eks_cluster.main
```

**âš ï¸ Warning:** This will delete:
- EKS cluster
- All databases (data loss!)
- ECR images
- DynamoDB tables
- All networking resources
---

